#!/usr/bin/env python3
"""
ç›‘æ§æŒ‡æ ‡åˆ†æå·¥å…·
ç”¨äºåˆ†æå’ŒéªŒè¯Prometheusç›‘æ§æŒ‡æ ‡çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
"""

import argparse
import time
from collections import defaultdict
from datetime import datetime
from typing import Any, Dict, List

import requests


class MetricsAnalyzer:
    """ç›‘æ§æŒ‡æ ‡åˆ†æå™¨"""

    def __init__(self, base_url: str):
        self.base_url = base_url.rstrip("/")
        self.metrics_history: List[Dict[str, Any]] = []

        # å…³é”®æŒ‡æ ‡å®šä¹‰
        self.key_metrics = {
            "webhook_requests_total": {
                "type": "counter",
                "description": "Webhookè¯·æ±‚æ€»æ•°",
                "labels": ["provider", "event_type", "status"],
            },
            "webhook_request_duration_seconds": {
                "type": "histogram",
                "description": "Webhookè¯·æ±‚è€—æ—¶",
                "labels": ["provider", "event_type"],
            },
            "idempotency_checks_total": {
                "type": "counter",
                "description": "å¹‚ç­‰æ€§æ£€æŸ¥æ€»æ•°",
                "labels": ["provider", "result"],
            },
            "duplicate_events_total": {
                "type": "counter",
                "description": "é‡å¤äº‹ä»¶æ€»æ•°",
                "labels": ["provider", "duplicate_type"],
            },
            "sync_events_total": {
                "type": "counter",
                "description": "åŒæ­¥äº‹ä»¶æ€»æ•°",
                "labels": [
                    "source_platform",
                    "target_platform",
                    "entity_type",
                    "action",
                    "status",
                ],
            },
            "github_api_calls_total": {
                "type": "counter",
                "description": "GitHub APIè°ƒç”¨æ€»æ•°",
                "labels": ["method", "endpoint", "status_code"],
            },
            "notion_api_calls_total": {
                "type": "counter",
                "description": "Notion APIè°ƒç”¨æ€»æ•°",
                "labels": ["method", "endpoint", "status_code"],
            },
        }

    def fetch_metrics(self) -> Dict[str, Any]:
        """è·å–å½“å‰çš„ç›‘æ§æŒ‡æ ‡"""
        try:
            response = requests.get(f"{self.base_url}/metrics", timeout=10)
            if response.status_code == 200:
                return {
                    "timestamp": datetime.now(),
                    "status": "success",
                    "raw_text": response.text,
                    "parsed_metrics": self.parse_prometheus_metrics(response.text),
                }
            else:
                return {
                    "timestamp": datetime.now(),
                    "status": "error",
                    "error": f"HTTP {response.status_code}",
                    "raw_text": "",
                    "parsed_metrics": {},
                }
        except Exception as e:
            return {
                "timestamp": datetime.now(),
                "status": "error",
                "error": str(e),
                "raw_text": "",
                "parsed_metrics": {},
            }

    def parse_prometheus_metrics(self, metrics_text: str) -> Dict[str, Dict[str, Any]]:
        """è§£æPrometheusæ ¼å¼çš„æŒ‡æ ‡"""
        metrics = defaultdict(lambda: {"values": [], "help": "", "type": ""})

        for line in metrics_text.split("\n"):
            line = line.strip()
            if not line:
                continue

            # è§£æHELPæ³¨é‡Š
            if line.startswith("# HELP "):
                parts = line[7:].split(" ", 1)
                if len(parts) >= 2:
                    metric_name, help_text = parts[0], parts[1]
                    metrics[metric_name]["help"] = help_text

            # è§£æTYPEæ³¨é‡Š
            elif line.startswith("# TYPE "):
                parts = line[7:].split(" ", 1)
                if len(parts) >= 2:
                    metric_name, metric_type = parts[0], parts[1]
                    metrics[metric_name]["type"] = metric_type

            # è·³è¿‡å…¶ä»–æ³¨é‡Š
            elif line.startswith("#"):
                continue

            # è§£ææŒ‡æ ‡å€¼
            else:
                try:
                    if " " in line:
                        metric_part, value_part = line.rsplit(" ", 1)
                        value = float(value_part)

                        # è§£ææŒ‡æ ‡åç§°å’Œæ ‡ç­¾
                        if "{" in metric_part and "}" in metric_part:
                            metric_name = metric_part.split("{")[0]
                            labels_str = metric_part[metric_part.index("{") + 1 : metric_part.rindex("}")]
                            labels = self.parse_labels(labels_str)
                        else:
                            metric_name = metric_part
                            labels = {}

                        metrics[metric_name]["values"].append({"labels": labels, "value": value})

                except (ValueError, IndexError):
                    continue

        return dict(metrics)

    def parse_labels(self, labels_str: str) -> Dict[str, str]:
        """è§£æPrometheusæ ‡ç­¾"""
        labels = {}
        if not labels_str:
            return labels

        # ç®€å•çš„æ ‡ç­¾è§£æ
        for label_pair in labels_str.split(","):
            label_pair = label_pair.strip()
            if "=" in label_pair:
                key, value = label_pair.split("=", 1)
                key = key.strip()
                value = value.strip().strip('"')
                labels[key] = value

        return labels

    def analyze_metrics_completeness(self, parsed_metrics: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææŒ‡æ ‡å®Œæ•´æ€§"""
        found_metrics = set(parsed_metrics.keys())
        expected_metrics = set(self.key_metrics.keys())

        missing_metrics = expected_metrics - found_metrics
        unexpected_metrics = found_metrics - expected_metrics

        # æ£€æŸ¥æ¯ä¸ªæŒ‡æ ‡çš„æ ‡ç­¾å®Œæ•´æ€§
        label_analysis = {}
        for metric_name, metric_data in parsed_metrics.items():
            if metric_name in self.key_metrics:
                expected_labels = set(self.key_metrics[metric_name].get("labels", []))

                actual_labels = set()
                for value_entry in metric_data["values"]:
                    actual_labels.update(value_entry["labels"].keys())

                label_analysis[metric_name] = {
                    "expected_labels": list(expected_labels),
                    "actual_labels": list(actual_labels),
                    "missing_labels": list(expected_labels - actual_labels),
                    "unexpected_labels": list(actual_labels - expected_labels),
                }

        return {
            "total_metrics": len(found_metrics),
            "expected_metrics": len(expected_metrics),
            "found_key_metrics": len(found_metrics & expected_metrics),
            "missing_metrics": list(missing_metrics),
            "unexpected_metrics": list(unexpected_metrics),
            "completeness_ratio": (
                len(found_metrics & expected_metrics) / len(expected_metrics) if expected_metrics else 1.0
            ),
            "label_analysis": label_analysis,
        }

    def analyze_metrics_values(self, parsed_metrics: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææŒ‡æ ‡æ•°å€¼åˆç†æ€§"""
        analysis = {}

        for metric_name, metric_data in parsed_metrics.items():
            if metric_name not in self.key_metrics:
                continue

            values = [entry["value"] for entry in metric_data["values"]]
            if not values:
                continue

            metric_type = self.key_metrics[metric_name]["type"]

            analysis[metric_name] = {
                "type": metric_type,
                "total_series": len(metric_data["values"]),
                "total_value": sum(values),
                "max_value": max(values),
                "min_value": min(values),
                "avg_value": sum(values) / len(values),
                "non_zero_series": sum(1 for v in values if v > 0),
                "zero_series": sum(1 for v in values if v == 0),
            }

            # å¯¹äºcounterç±»å‹ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰è´Ÿå€¼ï¼ˆä¸åˆç†ï¼‰
            if metric_type == "counter":
                negative_values = [v for v in values if v < 0]
                analysis[metric_name]["negative_values"] = len(negative_values)
                analysis[metric_name]["has_negative"] = len(negative_values) > 0

        return analysis

    def collect_metrics_over_time(self, duration_minutes: int = 5, interval_seconds: int = 30) -> List[Dict[str, Any]]:
        """åœ¨ä¸€æ®µæ—¶é—´å†…æ”¶é›†æŒ‡æ ‡"""
        print(f"ğŸ“Š å¼€å§‹æ”¶é›†æŒ‡æ ‡ï¼ŒæŒç»­ {duration_minutes} åˆ†é’Ÿï¼Œé—´éš” {interval_seconds} ç§’...")

        end_time = time.time() + (duration_minutes * 60)
        metrics_snapshots = []

        while time.time() < end_time:
            metrics_data = self.fetch_metrics()
            metrics_snapshots.append(metrics_data)

            if metrics_data["status"] == "success":
                print(
                    f"âœ… {metrics_data['timestamp'].strftime('%H:%M:%S')} - æŒ‡æ ‡æ”¶é›†æˆåŠŸ "
                    f"({len(metrics_data['parsed_metrics'])} ä¸ªæŒ‡æ ‡)"
                )
            else:
                print(
                    f"âŒ {metrics_data['timestamp'].strftime('%H:%M:%S')} - æŒ‡æ ‡æ”¶é›†å¤±è´¥: "
                    f"{metrics_data.get('error', 'Unknown error')}"
                )

            time.sleep(interval_seconds)

        self.metrics_history.extend(metrics_snapshots)
        return metrics_snapshots

    def analyze_metrics_trends(self, metrics_snapshots: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææŒ‡æ ‡è¶‹åŠ¿"""
        if len(metrics_snapshots) < 2:
            return {"error": "éœ€è¦è‡³å°‘2ä¸ªæ—¶é—´ç‚¹çš„æ•°æ®æ¥åˆ†æè¶‹åŠ¿"}

        trends = {}

        # æå–æ¯ä¸ªæ—¶é—´ç‚¹çš„æŒ‡æ ‡æ€»å’Œ
        for metric_name in self.key_metrics.keys():
            metric_values_over_time = []

            for snapshot in metrics_snapshots:
                if snapshot["status"] != "success":
                    continue

                parsed_metrics = snapshot["parsed_metrics"]
                if metric_name in parsed_metrics:
                    total_value = sum(entry["value"] for entry in parsed_metrics[metric_name]["values"])
                    metric_values_over_time.append({"timestamp": snapshot["timestamp"], "value": total_value})

            if len(metric_values_over_time) >= 2:
                initial_value = metric_values_over_time[0]["value"]
                final_value = metric_values_over_time[-1]["value"]

                trends[metric_name] = {
                    "initial_value": initial_value,
                    "final_value": final_value,
                    "absolute_change": final_value - initial_value,
                    "relative_change": (
                        ((final_value - initial_value) / initial_value * 100) if initial_value > 0 else 0
                    ),
                    "data_points": len(metric_values_over_time),
                    "trend": (
                        "increasing"
                        if final_value > initial_value
                        else ("decreasing" if final_value < initial_value else "stable")
                    ),
                }

        return trends

    def generate_analysis_report(self, include_trends: bool = True) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if not self.metrics_history:
            return "âŒ æ²¡æœ‰å¯ç”¨çš„æŒ‡æ ‡æ•°æ®è¿›è¡Œåˆ†æ"

        # ä½¿ç”¨æœ€æ–°çš„æŒ‡æ ‡æ•°æ®è¿›è¡Œåˆ†æ
        latest_snapshot = None
        for snapshot in reversed(self.metrics_history):
            if snapshot["status"] == "success":
                latest_snapshot = snapshot
                break

        if not latest_snapshot:
            return "âŒ æ²¡æœ‰æˆåŠŸçš„æŒ‡æ ‡æ•°æ®å¿«ç…§"

        report = []
        report.append("=" * 80)
        report.append("ç›‘æ§æŒ‡æ ‡åˆ†ææŠ¥å‘Š")
        report.append("=" * 80)
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"æ•°æ®å¿«ç…§æ—¶é—´: {latest_snapshot['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"æ€»æ•°æ®ç‚¹: {len(self.metrics_history)}")
        report.append("")

        # å®Œæ•´æ€§åˆ†æ
        completeness = self.analyze_metrics_completeness(latest_snapshot["parsed_metrics"])
        report.append("ğŸ“‹ æŒ‡æ ‡å®Œæ•´æ€§åˆ†æ:")
        report.append(f"  æ‰¾åˆ°æŒ‡æ ‡: {completeness['total_metrics']}")
        report.append(f"  é¢„æœŸæ ¸å¿ƒæŒ‡æ ‡: {completeness['expected_metrics']}")
        report.append(
            f"  æ ¸å¿ƒæŒ‡æ ‡è¦†ç›–: {completeness['found_key_metrics']}/{completeness['expected_metrics']} "
            f"({completeness['completeness_ratio']:.1%})"
        )

        if completeness["missing_metrics"]:
            report.append(f"  ç¼ºå¤±æŒ‡æ ‡: {', '.join(completeness['missing_metrics'])}")

        if completeness["unexpected_metrics"]:
            report.append(f"  é¢å¤–æŒ‡æ ‡: {len(completeness['unexpected_metrics'])} ä¸ª")

        report.append("")

        # æ•°å€¼åˆ†æ
        values_analysis = self.analyze_metrics_values(latest_snapshot["parsed_metrics"])
        report.append("ğŸ“ˆ æŒ‡æ ‡æ•°å€¼åˆ†æ:")

        for metric_name, analysis in values_analysis.items():
            report.append(f"  {metric_name} ({analysis['type']}):")
            report.append(f"    æ—¶é—´åºåˆ—æ•°: {analysis['total_series']}")
            report.append(f"    æ€»å€¼: {analysis['total_value']:.2f}")
            report.append(f"    æœ€å¤§å€¼: {analysis['max_value']:.2f}")
            report.append(f"    å¹³å‡å€¼: {analysis['avg_value']:.2f}")
            report.append(f"    éé›¶åºåˆ—: {analysis['non_zero_series']}/{analysis['total_series']}")

            if analysis.get("has_negative"):
                report.append(f"    âš ï¸ å‘ç°è´Ÿå€¼: {analysis['negative_values']} ä¸ª (Counterç±»å‹ä¸åº”æœ‰è´Ÿå€¼)")

        report.append("")

        # è¶‹åŠ¿åˆ†æï¼ˆå¦‚æœæœ‰å¤šä¸ªæ•°æ®ç‚¹ï¼‰
        if include_trends and len(self.metrics_history) > 1:
            trends = self.analyze_metrics_trends(self.metrics_history)
            if "error" not in trends:
                report.append("ğŸ“Š æŒ‡æ ‡è¶‹åŠ¿åˆ†æ:")
                for metric_name, trend in trends.items():
                    report.append(f"  {metric_name}:")
                    report.append(f"    è¶‹åŠ¿: {trend['trend']}")
                    report.append(f"    å˜åŒ–é‡: {trend['absolute_change']:.2f} ({trend['relative_change']:+.1f}%)")
                    report.append(f"    åˆå§‹å€¼: {trend['initial_value']:.2f}")
                    report.append(f"    æœ€ç»ˆå€¼: {trend['final_value']:.2f}")
                report.append("")

        # æ ‡ç­¾åˆ†æ
        label_analysis = completeness.get("label_analysis", {})
        if label_analysis:
            report.append("ğŸ·ï¸ æ ‡ç­¾å®Œæ•´æ€§åˆ†æ:")
            for metric_name, labels in label_analysis.items():
                if labels["missing_labels"] or labels["unexpected_labels"]:
                    report.append(f"  {metric_name}:")
                    if labels["missing_labels"]:
                        report.append(f"    ç¼ºå¤±æ ‡ç­¾: {', '.join(labels['missing_labels'])}")
                    if labels["unexpected_labels"]:
                        report.append(f"    é¢å¤–æ ‡ç­¾: {', '.join(labels['unexpected_labels'])}")
            report.append("")

        # è¯„ä¼°æ€»ç»“
        report.append("ğŸ¯ è¯„ä¼°æ€»ç»“:")

        # å®Œæ•´æ€§è¯„åˆ†
        completeness_score = completeness["completeness_ratio"] * 100
        if completeness_score >= 90:
            report.append("  âœ… æŒ‡æ ‡å®Œæ•´æ€§: ä¼˜ç§€")
        elif completeness_score >= 70:
            report.append("  âš ï¸ æŒ‡æ ‡å®Œæ•´æ€§: è‰¯å¥½")
        else:
            report.append("  âŒ æŒ‡æ ‡å®Œæ•´æ€§: éœ€è¦æ”¹è¿›")

        # æ•°æ®è´¨é‡è¯„åˆ†
        total_metrics = len(values_analysis)
        problematic_metrics = sum(1 for analysis in values_analysis.values() if analysis.get("has_negative", False))

        if problematic_metrics == 0:
            report.append("  âœ… æ•°æ®è´¨é‡: ä¼˜ç§€")
        elif problematic_metrics <= total_metrics * 0.1:
            report.append("  âš ï¸ æ•°æ®è´¨é‡: è‰¯å¥½")
        else:
            report.append("  âŒ æ•°æ®è´¨é‡: éœ€è¦å…³æ³¨")

        report.append("=" * 80)

        return "\n".join(report)

    def run_comprehensive_analysis(self, duration_minutes: int = 5) -> bool:
        """è¿è¡Œç»¼åˆåˆ†æ"""
        print(f"ğŸ” å¼€å§‹ç›‘æ§æŒ‡æ ‡ç»¼åˆåˆ†æ (æŒç»­ {duration_minutes} åˆ†é’Ÿ)")
        print(f"ğŸ“ ç›®æ ‡æœåŠ¡: {self.base_url}")
        print("")

        # é¦–å…ˆæ£€æŸ¥æŒ‡æ ‡ç«¯ç‚¹æ˜¯å¦å¯ç”¨
        initial_metrics = self.fetch_metrics()
        if initial_metrics["status"] != "success":
            print(f"âŒ æ— æ³•è®¿é—®æŒ‡æ ‡ç«¯ç‚¹: {initial_metrics.get('error', 'Unknown error')}")
            return False

        print(f"âœ… æŒ‡æ ‡ç«¯ç‚¹å¯è®¿é—®ï¼Œå‘ç° {len(initial_metrics['parsed_metrics'])} ä¸ªæŒ‡æ ‡")

        # æ”¶é›†ä¸€æ®µæ—¶é—´å†…çš„æŒ‡æ ‡
        metrics_snapshots = self.collect_metrics_over_time(duration_minutes)

        # ç”Ÿæˆå¹¶æ‰“å°æŠ¥å‘Š
        report = self.generate_analysis_report()
        print("\n" + report)

        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_filename = f"metrics_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_filename, "w", encoding="utf-8") as f:
            f.write(report)

        print(f"ğŸ“‹ åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_filename}")

        # ç®€å•çš„æˆåŠŸè¯„ä¼°
        successful_snapshots = sum(1 for s in metrics_snapshots if s["status"] == "success")
        success_rate = successful_snapshots / len(metrics_snapshots) if metrics_snapshots else 0

        if success_rate >= 0.9:
            print("ğŸ‰ ç›‘æ§æŒ‡æ ‡åˆ†æå®Œæˆï¼Œç³»ç»Ÿè¡¨ç°è‰¯å¥½")
            return True
        else:
            print(f"âš ï¸ ç›‘æ§æŒ‡æ ‡åˆ†æå®Œæˆï¼ŒæˆåŠŸç‡ {success_rate:.1%}ï¼Œéœ€è¦å…³æ³¨")
            return False


def main():
    parser = argparse.ArgumentParser(description="ç›‘æ§æŒ‡æ ‡åˆ†æå·¥å…·")
    parser.add_argument("--url", required=True, help="æœåŠ¡åŸºç¡€URL")
    parser.add_argument("--duration", type=int, default=5, help="ç›‘æ§æŒç»­æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰")
    parser.add_argument("--interval", type=int, default=30, help="é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰")
    parser.add_argument("--quick", action="store_true", help="å¿«é€Ÿåˆ†æï¼ˆåªåˆ†æå½“å‰çŠ¶æ€ï¼‰")

    args = parser.parse_args()

    analyzer = MetricsAnalyzer(args.url)

    try:
        if args.quick:
            # å¿«é€Ÿåˆ†æ
            print("âš¡ è¿è¡Œå¿«é€ŸæŒ‡æ ‡åˆ†æ...")
            metrics_data = analyzer.fetch_metrics()
            analyzer.metrics_history = [metrics_data]

            if metrics_data["status"] == "success":
                report = analyzer.generate_analysis_report(include_trends=False)
                print(report)
                return 0
            else:
                print(f"âŒ æŒ‡æ ‡è·å–å¤±è´¥: {metrics_data.get('error', 'Unknown error')}")
                return 1
        else:
            # å®Œæ•´åˆ†æ
            success = analyzer.run_comprehensive_analysis(args.duration)
            return 0 if success else 1

    except KeyboardInterrupt:
        print("\nâ¹ï¸ åˆ†æè¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"âŒ åˆ†ææ‰§è¡Œå¤±è´¥: {e}")
        return 1


if __name__ == "__main__":
    exit(main())
